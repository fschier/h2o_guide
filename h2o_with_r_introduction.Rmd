---
title: "H2O AutoML Regression Demo"
output:
  html_document: default
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. To execute a code chunk, click *Run* (play) button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

If you're viewing the Rmd file (code only), but you'd like to see the code *and* output rendered as an HTML document, an online HTML of this file is available [here](http://htmlpreview.github.io/?https://github.com/h2oai/h2o-tutorials/blob/master/h2o-world-2017/automl/R/automl_regression_powerplant_output.html).

### Install H2O

```{r}
# # The following two commands remove any previously installed H2O packages for R.
# if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
# if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }
# 
# # Next, we download packages that H2O depends on.
# pkgs <- c("RCurl","jsonlite")
# for (pkg in pkgs) {
# if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
# }
# 
# # Now we download, install and initialize the H2O package for R.
# install.packages("h2o", type="source", repos="http://h2o-release.s3.amazonaws.com/h2o/rel-zermelo/3/R")
# 
# # Finally, let's load H2O and start up an H2O cluster
# library(h2o)
# h2o.init()
```


### Start H2O

Load the **h2o** R library and initialize a local H2O cluster.

```{r}
library(tidyverse)
library(h2o)
h2o.init()
#h2o.no_progress()  # Turn off progress bars for notebook readability
#h2o.shutdown(prompt = TRUE)
#h2o.shutdown(prompt = TRUE)
```

### Load Data

For the AutoML regression demo, we use the [Combined Cycle Power Plant](http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant) dataset.  The goal here is to predict the energy output (in megawatts), given the temperature, ambient pressure, relative humidity and exhaust vacuum values.  In this demo, you will use H2O's AutoML to outperform the [state of the art results](https://www.sciencedirect.com/science/article/pii/S0142061514000908) on this task.

```{r}
# Use local data file or download from GitHub
docker_data_path <- "/home/h2o/data/automl/powerplant_output.csv"
if (file.exists(docker_data_path)) {
  data_path <- docker_data_path
} else {
  data_path <- "https://github.com/h2oai/h2o-tutorials/raw/master/h2o-world-2017/automl/data/powerplant_output.csv"
}
# Load data into H2O
df <- h2o.importFile(data_path)

```


Let's take a look at the data.
```{r}
h2o.describe(df)
```

Next, let's identify the response column and save the column name as `y`.  In this dataset, we will use all columns except the response as predictors, so we can skip setting the `x` argument explicitly.
```{r}
y <- "HourlyEnergyOutputMW"
```

Lastly, let's split the data into two frames, a `train` (80%) and a `test` frame (20%).  The `test` frame will be used to score the leaderboard and to demonstrate how to generate predictions using an AutoML leader model.
```{r}
splits <- h2o.splitFrame(df, ratios = 0.8, seed = 1)
train <- splits[[1]]
test <- splits[[2]]
```


## Run AutoML 

Run AutoML, stopping after 60 seconds.  The `max_runtime_secs` argument provides a way to limit the AutoML run by time.  When using a time-limited stopping criterion, the number of models train will vary between runs.  If different hardware is used or even if the same machine is used but the available compute resources on that machine are not the same between runs, then AutoML may be able to train more models on one run vs another. 

The `test` frame is passed explicitly to the `leaderboard_frame` argument here, which means that instead of using cross-validated metrics, we use test set metrics for generating the leaderboard.
```{r}
aml <- h2o.automl(y = y,
                  training_frame = train,
                  leaderboard_frame = test,
                  max_runtime_secs = 60,
                  seed = 1,
                  project_name = "powerplant_lb_frame")
```


For demonstration purposes, we will also execute a second AutoML run, this time providing the original, full dataset, `df` (without passing a `leaderboard_frame`).  This is a more efficient use of our data since we can use 100% of the data for training, rather than 80% like we did above.  This time our leaderboard will use cross-validated metrics.

*Note: Using an explicit `leaderboard_frame` for scoring may be useful in some cases, which is why the option is available.*  

```{r}
aml2 <- h2o.automl(y = y,
                   training_frame = df,
                   max_runtime_secs = 60,
                   seed = 1,
                   project_name = "powerplant_full_data")
```

*Note: We specify a `project_name` here for clarity.*

## Leaderboard

Next, we will view the AutoML Leaderboard.  Since we specified a `leaderboard_frame` in the `h2o.automl()` function for scoring and ranking the models, the AutoML leaderboard uses the performance on this data to rank the models.  

After viewing the `"powerplant_lb_frame"` AutoML project leaderboard, we compare that to the leaderboard for the `"powerplant_full_data"` project.  We can see that the results are better when the full dataset is used for training.  

A default performance metric for each machine learning task (binary classification, multiclass classification, regression) is specified internally and the leaderboard will be sorted by that metric.  In the case of regression, the default ranking metric is mean residual deviance.  In the future, the user will be able to specify any of the H2O metrics so that different metrics can be used to generate rankings on the leaderboard.
```{r}
print(aml@leaderboard)
```

```{r}
print(aml2@leaderboard)
```

This dataset comes from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant) of machine learning datasets.  The data was used in a [publication](https://www.sciencedirect.com/science/article/pii/S0142061514000908) in the *International Journal of Electrical Power & Energy Systems* in 2014.  In the paper, the authors achieved a mean absolute error (MAE) of 2.818 and a Root Mean-Squared Error (RMSE) of 3.787 on their best model.  So, with H2O's AutoML, we've already beaten the state-of-the-art in just 60 seconds of compute time!

## Predict Using Leader Model

If you need to generate predictions on a test set, you can make predictions on the `"H2OAutoML"` object directly, or on the leader model object.
```{r}
pred <- h2o.predict(aml, test)  # predict(aml, test) and h2o.predict(aml@leader, test) also work
head(pred)

predicted_values <- pred %>%
   as.data.frame()

# Plot the result of predicted vs actual value! (Felix)

test  %>% 
  as.data.frame() %>%
  select(value = HourlyEnergyOutputMW) %>%
  cbind(predicted_values) %>%
  ggplot(aes(value, predict)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0)
```


If needed, the standard `h2o.performance()` function can be applied to the AutoML leader model and a test set to generate an H2O model performance object.
```{r}
perf <- h2o.performance(aml@leader, test)
perf
```

## H2o Demo: K-means

```{r}
#h20 DEMO

#library(h2o)
#localH2O = h2o.init()
#demo(h2o.kmeans)
```

## Using H2o for Employee Turnover

Source: https://www.business-science.io/business/2017/09/18/hr_employee_attrition.html

```{r}
#Using h2o on HR Data
library(h2o)
localH2O = h2o.init()

library(tidyverse)
library(readxl)
library(tidymodels)


attrition <- read_xlsx("00_Data/IBM_employee_attrition.xlsx")
glimpse(attrition)

attrition_clean <- attrition %>%
  select(-c("Over18","Attrition")) %>%
  rename(Attrition = Attrition_Chr) %>%
  mutate(across(is.character, as.factor)) %>%
  select(Attrition, everything()) %>%
  mutate(Attrition = fct_relevel(Attrition, c("Yes", "No")))

# For Tidymodels: creating dummy variables out of factors
# attrition_tidy <- attrition_clean %>%
#   recipe(Attrition ~ .) %>%
#   step_dummy(all_nominal()) %>%
#   prep() %>%
#   juice()
# 
# 
# glimpse(attrition_clean)

write_csv(attrition_clean, file.path(getwd(), "00_Data", "attrition_clean.csv"))

```

```{r}
# h2o Workflow with Attrition Data

library(h2o)
h2o.init()

#df_attrition <- h2o.importFile("00_Data/attrition_clean.csv") read directly from file

# read from R session
df_attrition <- as.h2o(attrition_clean)

# Split the dataset

split_h2o <- h2o.splitFrame(df_attrition, c(0.7, 0.15), seed = 1234 )

train_h2o <- h2o.assign(split_h2o[[1]], "train" ) # 70%
valid_h2o <- h2o.assign(split_h2o[[2]], "valid" ) # 15%
test_h2o  <- h2o.assign(split_h2o[[3]], "test" )  # 15%

# Specify Outcome Variable:
y <- "Attrition"
x <- setdiff(names(train_h2o), y)

```

Let's take a look at the data.
```{r}
h2o.describe(train_h2o)
```

```{r}
aml <- h2o.automl(
    x = x, # when x not provided, defaults to all column names excep y
    y = y,
    training_frame    = train_h2o,
    leaderboard_frame = valid_h2o,
    max_runtime_secs  = 30,
    project_name = "employee_attrition")
```


```{r}
# aml2 <- h2o.automl(y = y,
#                    x = x,
#                    training_frame = df_attrition,
#                    max_runtime_secs = 60,
#                    seed = 1,
#                    project_name = "employee_attrition_full_data")
```


```{r}
# print the leaderboard
print(aml@leaderboard)

# Extract leader model
automl_leader <- aml@leader
print(automl_leader)

# Leadermodel: Modeltype (later needed for LIME Package)
class(automl_leader)
```



```{r}
pred <- h2o.predict(object = aml, newdata =  test_h2o)  # predict(aml, test) and h2o.predict(aml@leader, test) also work
head(pred)

predicted_values <- pred %>%
   as.data.frame()

# Data Wrangling on the predicted values

test_performance <- test_h2o %>%
    tibble::as_tibble() %>%
    select(Attrition) %>%
    add_column(pred = as.vector(pred$predict)) %>%
    mutate_if(is.character, as.factor)

#Data Frame
test_performance

#Table aka COnfusion Matrix
confusion_matrix <- test_performance %>%
    table() 


# Classification Analysis with common metrics:

tn <- confusion_matrix[1]
tp <- confusion_matrix[4]
fp <- confusion_matrix[3]
fn <- confusion_matrix[2]

accuracy <- (tp + tn) / (tp + tn + fp + fn)
misclassification_rate <- 1 - accuracy
recall <- tp / (tp + fn)
precision <- tp / (tp + fp)
null_error_rate <- tn / (tp + tn + fp + fn)

tibble(
    accuracy,
    misclassification_rate,
    recall,
    precision,
    null_error_rate
) %>% 
    transpose() 

```


If needed, the standard `h2o.performance()` function can be applied to the AutoML leader model and a test set to generate an H2O model performance object.
```{r}
perf <- h2o.performance(aml@leader, test_h2o)
perf
```


Making Black Box Models explainable with LIME Package:
```{r}
library(lime)

# Setup lime::model_type() function for h2o
# model_type.H2OBinomialModel <- function(x, ...) {
#     # Function tells lime() what model type we are dealing with
#     # 'classification', 'regression', 'survival', 'clustering', 'multilabel', etc
#     #
#     # x is our h2o model
#     
#     return("classification")
# }
# 
# # Setup lime::predict_model() function for h2o
# predict_model.H2OBinomialModel <- function(x, newdata, type, ...) {
#     # Function performs prediction and returns dataframe with Response
#     #
#     # x is h2o model
#     # newdata is data frame
#     # type is only setup for data frame
#     
#     pred <- h2o.predict(x, as.h2o(newdata))
#     
#     # return probs
#     return(as.data.frame(pred[,-1]))
#     
# }

# Test our predict_model() function
predict_model(x = automl_leader, newdata = as.data.frame(test_h2o[,-1]), type = 'raw') %>%
    tibble::as_tibble()
```
```{r}
explainer <- lime::lime(
    as.data.frame(train_h2o[,-1]), 
    model          = automl_leader, 
    bin_continuous = FALSE)

explanation <- lime::explain(
    as.data.frame(test_h2o[1:10,-1]), 
    explainer    = explainer, 
    n_labels     = 1, 
    n_features   = 4,
    kernel_width = 0.5)
```

```{r}
plot_features(explanation) +
    labs(title = "HR Predictive Analytics: LIME Feature Importance Visualization",
         subtitle = "Hold Out (Test) Set, First 10 Cases Shown")

explanation

# Diving deeper into lime package and visualisation practices for models (h2o, caret, ...)
# https://www.business-science.io/business/2018/06/25/lime-local-feature-interpretation.html
plot_explanations(explanation)
```


```{r}
# Same Analysis with data preprosessed before passing it into h2o
attrition_clean %>% glimpse()

recipe_obj <- attrition_clean %>%
  recipe(formula = Attrition ~ .) %>%
  step_rm(EmployeeNumber) %>%
  step_zv(all_predictors()) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  prep(data = attrition_clean)

hr_data_bake_tbl <- bake(object = recipe_obj, new_data = attrition_clean) 

# Make "Yes" as first lvl of Factor, aka what we want to predict
hr_data_bake_tbl <- hr_data_bake_tbl %>%
  mutate(Attrition = fct_relevel(Attrition, c("Yes", "No")))

# h2o workflow

hr_data_bake_h2o <- as.h2o(hr_data_bake_tbl)

hr_data_split <- h2o.splitFrame(hr_data_bake_h2o, ratios = c(0.7, 0.3), seed = 1234)

train_h2o <- h2o.assign(hr_data_split[[1]], "train" ) # 70%
valid_h2o <- h2o.assign(hr_data_split[[2]], "valid" ) # 15%
test_h2o  <- h2o.assign(hr_data_split[[3]], "test" )  # 15%

# Run Model
y <- "Attrition"
x <- setdiff(names(train_h2o), y)

automl_models_h2o <- h2o.automl(
  x = x, 
  y = y,
  training_frame    = train_h2o,
  validation_frame  = valid_h2o,
  leaderboard_frame = test_h2o,
  max_runtime_secs  = 30
)

#Leader and Leaderboard

automl_leader <- automl_models_h2o@leader

automl_models_h2o@leaderboard

# Lime Package:
explainer <- lime::lime(
  as.data.frame(train_h2o[,-1]), 
  model          = automl_leader, 
  bin_continuous = FALSE
)

explanation <- lime::explain(
  x              = as.data.frame(test_h2o[1:10,-1]), 
  explainer      = explainer, 
  n_labels       = 1, 
  n_features     = 4,
  n_permutations = 500,
  kernel_width   = 1
)

explanation

plot_features(explanation) +
    labs(title = "HR Predictive Analytics: LIME Feature Importance Visualization",
         subtitle = "Hold Out (Test) Set, First 10 Cases Shown")
```

